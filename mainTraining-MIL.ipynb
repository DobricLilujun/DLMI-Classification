{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIL - Multi instance Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import copy\n",
    "import re\n",
    "import yaml\n",
    "import uuid\n",
    "import warnings\n",
    "import time\n",
    "import inspect\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import partial, reduce\n",
    "from random import shuffle\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import resnet\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, Resize\n",
    "from torchvision.models.resnet import ResNet, BasicBlock\n",
    "from torchvision.datasets import MNIST\n",
    "from tqdm.autonotebook import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn import metrics as mtx\n",
    "from sklearn import model_selection as ms\n",
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "\n",
    "def calculate_age(dob_str):\n",
    "    formats = [\"%m/%d/%Y\", \"%d-%m-%Y\"]  # Two different format at the same time!\n",
    "    for format_string in formats:\n",
    "        try:\n",
    "            dob_date = datetime.strptime(dob_str, format_string)\n",
    "            current_date = datetime.now()\n",
    "            age = (\n",
    "                current_date.year\n",
    "                - dob_date.year\n",
    "                - (\n",
    "                    (current_date.month, current_date.day)\n",
    "                    < (dob_date.month, dob_date.day)\n",
    "                )\n",
    "            )\n",
    "            return age\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "\n",
    "def encode_gender(gender):\n",
    "    if gender == \"F\" or gender == \"f\":  # Two different gender value F and f ....\n",
    "        return 0\n",
    "    elif gender == \"M\" or gender == \"m\":\n",
    "        return 1\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Invalid gender value. Expected 'F' or 'M', but received: {}\".format(gender)\n",
    "        )\n",
    "\n",
    "\n",
    "class DLMICustomDataset(Dataset):\n",
    "    def __init__(self, data, transform=None, flag=\"trainset\", max_images=150):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "        self.flag = flag\n",
    "        self.max_images = max_images\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        patient_ID = self.data.iloc[idx, 1]\n",
    "        img_path_folder = \"data/raw/\" + self.flag + \"/\" + str(patient_ID) + \"/\"\n",
    "\n",
    "        images = []\n",
    "        images_loaded = 0\n",
    "        for filename in os.listdir(img_path_folder):\n",
    "            if filename.endswith(\".jpg\"):\n",
    "                img_path = os.path.join(img_path_folder, filename)\n",
    "                image = Image.open(img_path).convert(\"RGB\")\n",
    "                if self.transform:\n",
    "                    image = self.transform(image)\n",
    "                images.append(image)\n",
    "                images_loaded += 1\n",
    "                if (\n",
    "                    images_loaded >= self.max_images\n",
    "                ):  # Stop loading if max_images reached\n",
    "                    break\n",
    "        if len(images) == 0:\n",
    "            images.append(torch.zeros((3, 224, 224)))  # Placeholder image\n",
    "\n",
    "        while len(images) < self.max_images:\n",
    "            random_image = random.choice(images)\n",
    "            images.append(random_image)\n",
    "\n",
    "        label = torch.tensor(int(self.data.iloc[idx, 2]), dtype=torch.float)\n",
    "        gender = torch.tensor(encode_gender(self.data.iloc[idx, 3]), dtype=torch.long)\n",
    "        age = torch.tensor(calculate_age(self.data.iloc[idx, 4]), dtype=torch.float32)\n",
    "        lymph_count = torch.tensor(self.data.iloc[idx, 5], dtype=torch.float32)\n",
    "        clinical_data = torch.stack((gender, age, lymph_count))\n",
    "        # (num_images, channels, height, width)\n",
    "        return torch.stack(images), clinical_data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridModel(nn.Module):\n",
    "    def __init__(self, num_classes, mlp_input_dim, mlp_hidden_dim):\n",
    "        super(HybridModel, self).__init__()\n",
    "        self.resnet18 = models.resnet18(pretrained=True)\n",
    "        num_ftrs = self.resnet18.fc.in_features\n",
    "        self.resnet18.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        self.mlp = MLP(\n",
    "            input_dim=mlp_input_dim, hidden_dim=mlp_hidden_dim, output_dim=num_classes\n",
    "        )\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.linear_classifier = nn.Linear(num_classes, 1)\n",
    "\n",
    "    def forward(self, image_data, clinical_data):\n",
    "        batch_size, num_images, channels, height, width = image_data.size()\n",
    "        image_data = image_data.view(-1, channels, height, width)\n",
    "        image_features = self.resnet18(image_data)\n",
    "        image_features = image_features.view(batch_size, num_images, -1)\n",
    "        image_features = self.pool(image_features)\n",
    "        image_output = F.sigmoid(self.linear_classifier(image_features))\n",
    "        clinical_output = F.sigmoid(self.mlp(clinical_data))\n",
    "        combined_output = (image_output + clinical_output) / 2\n",
    "        return combined_output\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import DatasetFolder\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "data = pd.read_csv(\n",
    "    \"/home/lujun/local/DLMI-Classification/data/raw/clinical_annotation.csv\"\n",
    ")\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "custom_dataset = DLMICustomDataset(data=data, transform=transform)\n",
    "\n",
    "train_size = int(0.8 * len(custom_dataset))\n",
    "val_size = len(custom_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(custom_dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "model = HybridModel(num_classes=1, mlp_input_dim=3, mlp_hidden_dim=16).to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, clinical, labels in train_loader:\n",
    "        inputs, clinical, labels = (\n",
    "            inputs.to(device),\n",
    "            clinical.to(device),\n",
    "            labels.to(device),\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, clinical)\n",
    "        labels = labels.unsqueeze(1)\n",
    "        labels.requires_grad = True\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, clinical, labels in val_loader:\n",
    "            inputs, clinical, labels = (\n",
    "                inputs.to(device),\n",
    "                clinical.to(device),\n",
    "                labels.to(device),\n",
    "            )\n",
    "            outputs = model(inputs, clinical)\n",
    "            labels = labels.unsqueeze(1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            total += labels.size(0)\n",
    "            correct += (outputs == labels).sum().item()\n",
    "    val_loss = val_loss / len(val_loader.dataset)  # Calculate validation loss\n",
    "    val_accuracy = correct / total  # Calculate validation accuracy\n",
    "    print(\n",
    "        f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"trained_model_original.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example image outputs: tensor([[-0.7063,  0.5859]], device='cuda:0')\n",
      "Example combined outputs: tensor([[-1.2863,  0.4067]], device='cuda:0')\n",
      "All Predictions: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set and output predictions\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "test_data = pd.read_csv(\n",
    "    \"/home/lujun/local/DLMI-Classification/data/post-processed/clinical_annotation_test.csv\"\n",
    ")\n",
    "test_dataset = DLMICustomDataset(\n",
    "    data=test_data, transform=transform, flag=\"testset_flattened\"\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    all_predictions = []  # Initialize list to store predictions\n",
    "    for i in range(len(test_dataset)):\n",
    "        inputs, clinical, _ = test_dataset[i]  # Get inputs, clinical data, and label\n",
    "        inputs, clinical = inputs.unsqueeze(0).to(device), clinical.unsqueeze(0).to(\n",
    "            device\n",
    "        )  # Add batch dimension and move to device\n",
    "        image_outputs, mlp_outputs = model(inputs, clinical)  # Forward pass\n",
    "        outputs = (image_outputs + mlp_outputs) / 2  # Average predictions\n",
    "        _, predicted = torch.max(outputs, 1)  # Get predicted class\n",
    "        all_predictions.append(predicted.item())  # Append predicted class to list\n",
    "\n",
    "        # Print example outputs for debugging\n",
    "        if i == 0:\n",
    "            print(\"Example image outputs:\", image_outputs)\n",
    "            print(\"Example combined outputs:\", outputs)\n",
    "\n",
    "# Output all predictions\n",
    "print(\"All Predictions:\", all_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\n",
    "    \"/home/lujun/local/DLMI-Classification/data/raw/clinical_annotation.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    113\n",
       " 0     50\n",
       "-1     42\n",
       "Name: LABEL, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.LABEL.value_counts()  # Unbalanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"Image_Name\": images_names, \"Prediction\": all_predictions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the count: 0 + 41\n",
      "This is the count: 0 + 69\n",
      "This is the count: 0 + 92\n",
      "This is the count: 0 + 103\n",
      "This is the count: 0 + 51\n",
      "This is the count: 0 + 137\n",
      "This is the count: 0 + 58\n",
      "This is the count: 0 + 112\n",
      "This is the count: 0 + 85\n",
      "This is the count: 0 + 166\n",
      "This is the count: 0 + 50\n",
      "This is the count: 0 + 46\n",
      "This is the count: 0 + 76\n",
      "This is the count: 0 + 59\n",
      "This is the count: 0 + 52\n",
      "This is the count: 0 + 145\n",
      "This is the count: 0 + 167\n",
      "This is the count: 0 + 134\n",
      "This is the count: 0 + 88\n",
      "This is the count: 0 + 142\n",
      "This is the count: 0 + 164\n",
      "This is the count: 0 + 42\n",
      "This is the count: 0 + 63\n",
      "This is the count: 0 + 69\n",
      "This is the count: 0 + 47\n",
      "This is the count: 0 + 180\n",
      "This is the count: 0 + 44\n",
      "This is the count: 0 + 71\n",
      "This is the count: 0 + 38\n",
      "This is the count: 0 + 115\n",
      "This is the count: 0 + 40\n",
      "This is the count: 0 + 33\n",
      "This is the count: 0 + 45\n",
      "This is the count: 0 + 42\n",
      "This is the count: 0 + 53\n",
      "This is the count: 0 + 50\n",
      "This is the count: 0 + 91\n",
      "This is the count: 0 + 51\n",
      "This is the count: 0 + 35\n",
      "This is the count: 0 + 55\n",
      "This is the count: 0 + 26\n",
      "This is the count: 0 + 31\n",
      "Person P152: Predicted Label 1\n",
      "Person P57: Predicted Label 1\n",
      "Person P108: Predicted Label 1\n",
      "Person P143: Predicted Label 1\n",
      "Person P49: Predicted Label 1\n",
      "Person P81: Predicted Label 1\n",
      "Person P18: Predicted Label 1\n",
      "Person P175: Predicted Label 1\n",
      "Person P69: Predicted Label 1\n",
      "Person P16: Predicted Label 1\n",
      "Person P170: Predicted Label 1\n",
      "Person P119: Predicted Label 1\n",
      "Person P9: Predicted Label 1\n",
      "Person P56: Predicted Label 1\n",
      "Person P71: Predicted Label 1\n",
      "Person P172: Predicted Label 1\n",
      "Person P75: Predicted Label 1\n",
      "Person P132: Predicted Label 1\n",
      "Person P203: Predicted Label 1\n",
      "Person P32: Predicted Label 1\n",
      "Person P120: Predicted Label 1\n",
      "Person P58: Predicted Label 1\n",
      "Person P133: Predicted Label 1\n",
      "Person P7: Predicted Label 1\n",
      "Person P197: Predicted Label 1\n",
      "Person P195: Predicted Label 1\n",
      "Person P93: Predicted Label 1\n",
      "Person P138: Predicted Label 1\n",
      "Person P114: Predicted Label 1\n",
      "Person P92: Predicted Label 1\n",
      "Person P14: Predicted Label 1\n",
      "Person P148: Predicted Label 1\n",
      "Person P196: Predicted Label 1\n",
      "Person P98: Predicted Label 1\n",
      "Person P188: Predicted Label 1\n",
      "Person P4: Predicted Label 1\n",
      "Person P139: Predicted Label 1\n",
      "Person P178: Predicted Label 1\n",
      "Person P86: Predicted Label 1\n",
      "Person P24: Predicted Label 1\n",
      "Person P73: Predicted Label 1\n",
      "Person P68: Predicted Label 1\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "person_predictions = defaultdict(list)\n",
    "images_names = []\n",
    "source_folder_path = (\n",
    "    \"/home/lujun/local/DLMI-Classification/data/post-processed/testset_flattened\"\n",
    ")\n",
    "for person_folder in os.listdir(source_folder_path):\n",
    "    images_names.append(person_folder[:-4])\n",
    "\n",
    "for filename, prediction in zip(images_names, all_predictions):\n",
    "    person_number = filename.split(\"_\")[0]  # Extract person number from filename\n",
    "    person_predictions[person_number].append(prediction)\n",
    "\n",
    "person_labels = {}\n",
    "for person_number, predictions in person_predictions.items():\n",
    "    count_0 = predictions.count(0)\n",
    "    count_1 = predictions.count(1)\n",
    "    print(f\"This is the count: {count_0} + {count_1}\")\n",
    "    if count_0 > count_1:\n",
    "        person_labels[person_number] = 0\n",
    "    else:\n",
    "        person_labels[person_number] = 1\n",
    "\n",
    "# Output the final labels after majority voting\n",
    "for person_number, label in person_labels.items():\n",
    "    print(f\"Person {person_number}: Predicted Label {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3258"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['P59_000016',\n",
       " 'P96_000063',\n",
       " 'P23_000063',\n",
       " 'P33_000109',\n",
       " 'P0_000082',\n",
       " 'P6_000006',\n",
       " 'P135_000009',\n",
       " 'P79_000005',\n",
       " 'P90_000051',\n",
       " 'P94_000003',\n",
       " 'P40_000087',\n",
       " 'P128_000090',\n",
       " 'P101_000049',\n",
       " 'P85_000038',\n",
       " 'P112_000107',\n",
       " 'P111_000174',\n",
       " 'P168_000013',\n",
       " 'P156_000014',\n",
       " 'P28_000019',\n",
       " 'P126_000096',\n",
       " 'P166_000049',\n",
       " 'P96_000026',\n",
       " 'P113_000018',\n",
       " 'P121_000039',\n",
       " 'P123_000113',\n",
       " 'P174_000052',\n",
       " 'P30_000056',\n",
       " 'P157_000033',\n",
       " 'P102_000135',\n",
       " 'P26_000046',\n",
       " 'P79_000131',\n",
       " 'P171_000011',\n",
       " 'P43_000048',\n",
       " 'P194_000073',\n",
       " 'P198_000051',\n",
       " 'P37_000023',\n",
       " 'P35_000112',\n",
       " 'P97_000054',\n",
       " 'P55_000007',\n",
       " 'P67_000080',\n",
       " 'P165_000008',\n",
       " 'P2_000056',\n",
       " 'P112_000114',\n",
       " 'P25_000009',\n",
       " 'P15_000049',\n",
       " 'P40_000086',\n",
       " 'P94_000063',\n",
       " 'P21_000007',\n",
       " 'P134_000089',\n",
       " 'P90_000018',\n",
       " 'P37_000016',\n",
       " 'P59_000161',\n",
       " 'P168_000139',\n",
       " 'P65_000075',\n",
       " 'P162_000042',\n",
       " 'P23_000107',\n",
       " 'P126_000078',\n",
       " 'P147_000143',\n",
       " 'P44_000000',\n",
       " 'P85_000024',\n",
       " 'P136_000087',\n",
       " 'P5_000097',\n",
       " 'P134_000060',\n",
       " 'P147_000095',\n",
       " 'P137_000046',\n",
       " 'P77_000027',\n",
       " 'P111_000005',\n",
       " 'P85_000027',\n",
       " 'P94_000023',\n",
       " 'P35_000002',\n",
       " 'P27_000030',\n",
       " 'P123_000002',\n",
       " 'P48_000000',\n",
       " 'P85_000151',\n",
       " 'P35_000191',\n",
       " 'P199_000043',\n",
       " 'P48_000024',\n",
       " 'P112_000069',\n",
       " 'P147_000116',\n",
       " 'P200_000014',\n",
       " 'P183_000056',\n",
       " 'P27_000041',\n",
       " 'P85_000138',\n",
       " 'P46_000008',\n",
       " 'P48_000051',\n",
       " 'P134_000080',\n",
       " 'P199_000008',\n",
       " 'P159_000025',\n",
       " 'P43_000055',\n",
       " 'P168_000094',\n",
       " 'P204_000040',\n",
       " 'P6_000012',\n",
       " 'P112_000002',\n",
       " 'P0_000077',\n",
       " 'P150_000072',\n",
       " 'P50_000037',\n",
       " 'P126_000035',\n",
       " 'P33_000000',\n",
       " 'P173_000008',\n",
       " 'P198_000047',\n",
       " 'P165_000025',\n",
       " 'P96_000094',\n",
       " 'P0_000075',\n",
       " 'P163_000077',\n",
       " 'P153_000012',\n",
       " 'P42_000101',\n",
       " 'P137_000019',\n",
       " 'P60_000003',\n",
       " 'P158_000138',\n",
       " 'P155_000031',\n",
       " 'P22_000015',\n",
       " 'P179_000017',\n",
       " 'P121_000038',\n",
       " 'P150_000053',\n",
       " 'P90_000139',\n",
       " 'P174_000066',\n",
       " 'P186_000072',\n",
       " 'P201_000008',\n",
       " 'P21_000048',\n",
       " 'P134_000100',\n",
       " 'P11_000014',\n",
       " 'P150_000067',\n",
       " 'P190_000080',\n",
       " 'P180_000016',\n",
       " 'P94_000049',\n",
       " 'P22_000006',\n",
       " 'P90_000124',\n",
       " 'P85_000117',\n",
       " 'P65_000007',\n",
       " 'P123_000170',\n",
       " 'P48_000034',\n",
       " 'P110_000046',\n",
       " 'P5_000084',\n",
       " 'P83_000001',\n",
       " 'P33_000045',\n",
       " 'P20_000050',\n",
       " 'P126_000023',\n",
       " 'P157_000038',\n",
       " 'P189_000074',\n",
       " 'P201_000060',\n",
       " 'P31_000044',\n",
       " 'P136_000085',\n",
       " 'P153_000069',\n",
       " 'P145_000007',\n",
       " 'P85_000047',\n",
       " 'P135_000002',\n",
       " 'P87_000043',\n",
       " 'P113_000055',\n",
       " 'P184_000095',\n",
       " 'P33_000080',\n",
       " 'P125_000023',\n",
       " 'P201_000035',\n",
       " 'P79_000030',\n",
       " 'P131_000007',\n",
       " 'P155_000035',\n",
       " 'P113_000003',\n",
       " 'P189_000078',\n",
       " 'P113_000007',\n",
       " 'P112_000023',\n",
       " 'P150_000066',\n",
       " 'P52_000044',\n",
       " 'P174_000049',\n",
       " 'P151_000050',\n",
       " 'P33_000094',\n",
       " 'P3_000064',\n",
       " 'P186_000067',\n",
       " 'P180_000027',\n",
       " 'P46_000134',\n",
       " 'P43_000051',\n",
       " 'P131_000023',\n",
       " 'P26_000062',\n",
       " 'P126_000067',\n",
       " 'P158_000154',\n",
       " 'P21_000077',\n",
       " 'P0_000002',\n",
       " 'P6_000025',\n",
       " 'P27_000167',\n",
       " 'P204_000024',\n",
       " 'P102_000114',\n",
       " 'P35_000177',\n",
       " 'P84_000012',\n",
       " 'P163_000103',\n",
       " 'P21_000034',\n",
       " 'P201_000064',\n",
       " 'P35_000068',\n",
       " 'P38_000031',\n",
       " 'P48_000008',\n",
       " 'P1_000051',\n",
       " 'P62_000012',\n",
       " 'P19_000031',\n",
       " 'P122_000039',\n",
       " 'P19_000009',\n",
       " 'P41_000013',\n",
       " 'P42_000022',\n",
       " 'P11_000031',\n",
       " 'P42_000017',\n",
       " 'P182_000014',\n",
       " 'P199_000004',\n",
       " 'P181_000096',\n",
       " 'P23_000141',\n",
       " 'P97_000047',\n",
       " 'P158_000048',\n",
       " 'P59_000068',\n",
       " 'P126_000032',\n",
       " 'P155_000062',\n",
       " 'P85_000177',\n",
       " 'P34_000038',\n",
       " 'P46_000121',\n",
       " 'P135_000038',\n",
       " 'P149_000007',\n",
       " 'P67_000089',\n",
       " 'P189_000096',\n",
       " 'P70_000008',\n",
       " 'P65_000016',\n",
       " 'P158_000157',\n",
       " 'P104_000016',\n",
       " 'P27_000010',\n",
       " 'P117_000031',\n",
       " 'P15_000106',\n",
       " 'P180_000060',\n",
       " 'P157_000047',\n",
       " 'P37_000158',\n",
       " 'P42_000144',\n",
       " 'P141_000017',\n",
       " 'P117_000018',\n",
       " 'P21_000014',\n",
       " 'P129_000012',\n",
       " 'P183_000012',\n",
       " 'P184_000113',\n",
       " 'P158_000119',\n",
       " 'P163_000110',\n",
       " 'P90_000154',\n",
       " 'P33_000023',\n",
       " 'P33_000147',\n",
       " 'P180_000111',\n",
       " 'P10_000008',\n",
       " 'P124_000035',\n",
       " 'P5_000090',\n",
       " 'P155_000056',\n",
       " 'P67_000036',\n",
       " 'P84_000018',\n",
       " 'P106_000047',\n",
       " 'P2_000113',\n",
       " 'P154_000052',\n",
       " 'P186_000037',\n",
       " 'P150_000025',\n",
       " 'P85_000022',\n",
       " 'P111_000184',\n",
       " 'P0_000039',\n",
       " 'P201_000001',\n",
       " 'P83_000047',\n",
       " 'P136_000051',\n",
       " 'P158_000139',\n",
       " 'P151_000081',\n",
       " 'P128_000022',\n",
       " 'P182_000001',\n",
       " 'P90_000104',\n",
       " 'P82_000017',\n",
       " 'P38_000013',\n",
       " 'P30_000031',\n",
       " 'P184_000172',\n",
       " 'P80_000049',\n",
       " 'P117_000098',\n",
       " 'P37_000083',\n",
       " 'P130_000012',\n",
       " 'P183_000022',\n",
       " 'P184_000002',\n",
       " 'P157_000056',\n",
       " 'P46_000064',\n",
       " 'P11_000008',\n",
       " 'P53_000018',\n",
       " 'P65_000073',\n",
       " 'P83_000107',\n",
       " 'P128_000086',\n",
       " 'P88_000059',\n",
       " 'P66_000020',\n",
       " 'P96_000003',\n",
       " 'P6_000044',\n",
       " 'P21_000000',\n",
       " 'P48_000058',\n",
       " 'P111_000150',\n",
       " 'P53_000069',\n",
       " 'P6_000041',\n",
       " 'P117_000013',\n",
       " 'P163_000082',\n",
       " 'P2_000005',\n",
       " 'P190_000033',\n",
       " 'P65_000054',\n",
       " 'P26_000041',\n",
       " 'P134_000071',\n",
       " 'P35_000170',\n",
       " 'P126_000115',\n",
       " 'P117_000059',\n",
       " 'P39_000020',\n",
       " 'P66_000052',\n",
       " 'P46_000095',\n",
       " 'P96_000136',\n",
       " 'P85_000165',\n",
       " 'P99_000033',\n",
       " 'P15_000055',\n",
       " 'P40_000021',\n",
       " 'P156_000030',\n",
       " 'P46_000020',\n",
       " 'P121_000030',\n",
       " 'P124_000090',\n",
       " 'P87_000023',\n",
       " 'P174_000033',\n",
       " 'P201_000005',\n",
       " 'P121_000033',\n",
       " 'P35_000136',\n",
       " 'P60_000024',\n",
       " 'P112_000006',\n",
       " 'P204_000038',\n",
       " 'P21_000075',\n",
       " 'P184_000109',\n",
       " 'P40_000048',\n",
       " 'P115_000021',\n",
       " 'P80_000053',\n",
       " 'P78_000013',\n",
       " 'P109_000048',\n",
       " 'P25_000010',\n",
       " 'P46_000173',\n",
       " 'P163_000148',\n",
       " 'P124_000104',\n",
       " 'P162_000022',\n",
       " 'P112_000055',\n",
       " 'P109_000060',\n",
       " 'P59_000079',\n",
       " 'P180_000042',\n",
       " 'P112_000148',\n",
       " 'P124_000039',\n",
       " 'P149_000012',\n",
       " 'P33_000032',\n",
       " 'P171_000004',\n",
       " 'P48_000055',\n",
       " 'P89_000017',\n",
       " 'P44_000037',\n",
       " 'P147_000070',\n",
       " 'P35_000163',\n",
       " 'P181_000002',\n",
       " 'P42_000123',\n",
       " 'P183_000062',\n",
       " 'P60_000053',\n",
       " 'P79_000058',\n",
       " 'P61_000022',\n",
       " 'P177_000031',\n",
       " 'P134_000017',\n",
       " 'P126_000053',\n",
       " 'P194_000009',\n",
       " 'P40_000015',\n",
       " 'P2_000072',\n",
       " 'P163_000137',\n",
       " 'P158_000084',\n",
       " 'P141_000044',\n",
       " 'P154_000035',\n",
       " 'P202_000055',\n",
       " 'P97_000052',\n",
       " 'P3_000010',\n",
       " 'P13_000022',\n",
       " 'P20_000011',\n",
       " 'P167_000028',\n",
       " 'P11_000115',\n",
       " 'P117_000057',\n",
       " 'P130_000010',\n",
       " 'P20_000056',\n",
       " 'P44_000062',\n",
       " 'P1_000013',\n",
       " 'P189_000047',\n",
       " 'P15_000093',\n",
       " 'P79_000101',\n",
       " 'P181_000027',\n",
       " 'P26_000088',\n",
       " 'P12_000003',\n",
       " 'P15_000016',\n",
       " 'P177_000002',\n",
       " 'P128_000060',\n",
       " 'P136_000104',\n",
       " 'P65_000052',\n",
       " 'P136_000017',\n",
       " 'P150_000037',\n",
       " 'P118_000026',\n",
       " 'P21_000041',\n",
       " 'P123_000134',\n",
       " 'P168_000083',\n",
       " 'P5_000104',\n",
       " 'P59_000116',\n",
       " 'P110_000041',\n",
       " 'P128_000045',\n",
       " 'P134_000115',\n",
       " 'P174_000037',\n",
       " 'P41_000015',\n",
       " 'P115_000057',\n",
       " 'P109_000005',\n",
       " 'P194_000034',\n",
       " 'P79_000045',\n",
       " 'P156_000081',\n",
       " 'P184_000187',\n",
       " 'P52_000009',\n",
       " 'P48_000002',\n",
       " 'P47_000004',\n",
       " 'P184_000097',\n",
       " 'P137_000039',\n",
       " 'P37_000129',\n",
       " 'P19_000102',\n",
       " 'P149_000041',\n",
       " 'P74_000031',\n",
       " 'P124_000070',\n",
       " 'P66_000037',\n",
       " 'P183_000009',\n",
       " 'P8_000030',\n",
       " 'P167_000071',\n",
       " 'P128_000113',\n",
       " 'P181_000095',\n",
       " 'P10_000024',\n",
       " 'P153_000027',\n",
       " 'P46_000128',\n",
       " 'P59_000052',\n",
       " 'P105_000013',\n",
       " 'P82_000032',\n",
       " 'P134_000088',\n",
       " 'P76_000036',\n",
       " 'P82_000027',\n",
       " 'P129_000003',\n",
       " 'P192_000081',\n",
       " 'P70_000004',\n",
       " 'P128_000128',\n",
       " 'P62_000000',\n",
       " 'P128_000071',\n",
       " 'P107_000004',\n",
       " 'P158_000107',\n",
       " 'P59_000148',\n",
       " 'P0_000024',\n",
       " 'P184_000073',\n",
       " 'P167_000031',\n",
       " 'P90_000058',\n",
       " 'P29_000049',\n",
       " 'P99_000083',\n",
       " 'P46_000169',\n",
       " 'P96_000083',\n",
       " 'P183_000043',\n",
       " 'P21_000056',\n",
       " 'P11_000121',\n",
       " 'P134_000108',\n",
       " 'P180_000107',\n",
       " 'P162_000036',\n",
       " 'P161_000017',\n",
       " 'P38_000008',\n",
       " 'P160_000048',\n",
       " 'P106_000045',\n",
       " 'P29_000036',\n",
       " 'P136_000032',\n",
       " 'P158_000029',\n",
       " 'P165_000064',\n",
       " 'P184_000108',\n",
       " 'P26_000074',\n",
       " 'P105_000048',\n",
       " 'P2_000084',\n",
       " 'P85_000068',\n",
       " 'P190_000056',\n",
       " 'P1_000041',\n",
       " 'P55_000006',\n",
       " 'P42_000161',\n",
       " 'P193_000041',\n",
       " 'P184_000185',\n",
       " 'P35_000162',\n",
       " 'P27_000083',\n",
       " 'P117_000046',\n",
       " 'P87_000046',\n",
       " 'P66_000040',\n",
       " 'P1_000040',\n",
       " 'P181_000104',\n",
       " 'P189_000093',\n",
       " 'P111_000171',\n",
       " 'P104_000035',\n",
       " 'P111_000116',\n",
       " 'P168_000003',\n",
       " 'P84_000053',\n",
       " 'P145_000015',\n",
       " 'P158_000031',\n",
       " 'P190_000036',\n",
       " 'P184_000181',\n",
       " 'P111_000143',\n",
       " 'P173_000017',\n",
       " 'P94_000002',\n",
       " 'P70_000001',\n",
       " 'P110_000019',\n",
       " 'P118_000020',\n",
       " 'P124_000100',\n",
       " 'P130_000027',\n",
       " 'P158_000017',\n",
       " 'P33_000111',\n",
       " 'P189_000079',\n",
       " 'P150_000049',\n",
       " 'P23_000074',\n",
       " 'P33_000092',\n",
       " 'P184_000114',\n",
       " 'P102_000076',\n",
       " 'P122_000011',\n",
       " 'P161_000008',\n",
       " 'P164_000023',\n",
       " 'P135_000064',\n",
       " 'P111_000119',\n",
       " 'P167_000042',\n",
       " 'P134_000030',\n",
       " 'P5_000063',\n",
       " 'P59_000131',\n",
       " 'P80_000013',\n",
       " 'P102_000040',\n",
       " 'P193_000000',\n",
       " 'P97_000021',\n",
       " 'P180_000118',\n",
       " 'P109_000038',\n",
       " 'P43_000054',\n",
       " 'P26_000005',\n",
       " 'P60_000045',\n",
       " 'P17_000020',\n",
       " 'P122_000050',\n",
       " 'P90_000023',\n",
       " 'P167_000065',\n",
       " 'P12_000012',\n",
       " 'P11_000070',\n",
       " 'P96_000013',\n",
       " 'P79_000158',\n",
       " 'P180_000085',\n",
       " 'P135_000092',\n",
       " 'P136_000069',\n",
       " 'P35_000103',\n",
       " 'P144_000011',\n",
       " 'P186_000038',\n",
       " 'P99_000063',\n",
       " 'P123_000039',\n",
       " 'P44_000052',\n",
       " 'P167_000093',\n",
       " 'P23_000152',\n",
       " 'P140_000014',\n",
       " 'P123_000065',\n",
       " 'P79_000022',\n",
       " 'P147_000112',\n",
       " 'P105_000015',\n",
       " 'P134_000084',\n",
       " 'P111_000046',\n",
       " 'P158_000073',\n",
       " 'P100_000009',\n",
       " 'P37_000123',\n",
       " 'P116_000018',\n",
       " 'P41_000003',\n",
       " 'P163_000142',\n",
       " 'P85_000094',\n",
       " 'P126_000093',\n",
       " 'P19_000067',\n",
       " 'P94_000082',\n",
       " 'P135_000104',\n",
       " 'P11_000064',\n",
       " 'P21_000024',\n",
       " 'P131_000031',\n",
       " 'P104_000048',\n",
       " 'P89_000008',\n",
       " 'P27_000116',\n",
       " 'P5_000003',\n",
       " 'P123_000071',\n",
       " 'P38_000042',\n",
       " 'P95_000042',\n",
       " 'P33_000006',\n",
       " 'P61_000023',\n",
       " 'P145_000061',\n",
       " 'P189_000028',\n",
       " 'P104_000046',\n",
       " 'P70_000003',\n",
       " 'P189_000090',\n",
       " 'P23_000056',\n",
       " 'P189_000087',\n",
       " 'P189_000025',\n",
       " 'P144_000065',\n",
       " 'P37_000133',\n",
       " 'P3_000053',\n",
       " 'P12_000025',\n",
       " 'P201_000000',\n",
       " 'P153_000076',\n",
       " 'P151_000061',\n",
       " 'P27_000044',\n",
       " 'P42_000168',\n",
       " 'P154_000032',\n",
       " 'P51_000004',\n",
       " 'P102_000053',\n",
       " 'P171_000041',\n",
       " 'P153_000075',\n",
       " 'P135_000060',\n",
       " 'P204_000060',\n",
       " 'P43_000033',\n",
       " 'P112_000042',\n",
       " 'P88_000038',\n",
       " 'P141_000052',\n",
       " 'P113_000001',\n",
       " 'P117_000065',\n",
       " 'P153_000031',\n",
       " 'P112_000051',\n",
       " 'P26_000079',\n",
       " 'P112_000013',\n",
       " 'P46_000125',\n",
       " 'P59_000096',\n",
       " 'P15_000027',\n",
       " 'P182_000004',\n",
       " 'P112_000158',\n",
       " 'P204_000022',\n",
       " 'P85_000109',\n",
       " 'P102_000086',\n",
       " 'P101_000016',\n",
       " 'P128_000010',\n",
       " 'P48_000014',\n",
       " 'P163_000080',\n",
       " 'P97_000050',\n",
       " 'P102_000142',\n",
       " 'P147_000032',\n",
       " 'P192_000002',\n",
       " 'P30_000070',\n",
       " 'P5_000031',\n",
       " 'P3_000048',\n",
       " 'P77_000012',\n",
       " 'P183_000018',\n",
       " 'P30_000079',\n",
       " 'P66_000061',\n",
       " 'P146_000023',\n",
       " 'P150_000055',\n",
       " 'P140_000009',\n",
       " 'P83_000030',\n",
       " 'P171_000010',\n",
       " 'P105_000004',\n",
       " 'P167_000041',\n",
       " 'P202_000043',\n",
       " 'P83_000042',\n",
       " 'P156_000049',\n",
       " 'P158_000012',\n",
       " 'P59_000145',\n",
       " 'P85_000160',\n",
       " 'P176_000001',\n",
       " 'P76_000039',\n",
       " 'P5_000043',\n",
       " 'P74_000024',\n",
       " 'P45_000042',\n",
       " 'P151_000012',\n",
       " 'P190_000023',\n",
       " 'P11_000104',\n",
       " 'P190_000093',\n",
       " 'P190_000061',\n",
       " 'P167_000021',\n",
       " 'P59_000011',\n",
       " 'P99_000010',\n",
       " 'P59_000129',\n",
       " 'P180_000126',\n",
       " 'P142_000054',\n",
       " 'P201_000014',\n",
       " 'P65_000010',\n",
       " 'P102_000071',\n",
       " 'P111_000141',\n",
       " 'P187_000040',\n",
       " 'P48_000063',\n",
       " 'P21_000073',\n",
       " 'P27_000065',\n",
       " 'P59_000041',\n",
       " 'P30_000061',\n",
       " 'P204_000054',\n",
       " 'P112_000074',\n",
       " 'P0_000074',\n",
       " 'P52_000038',\n",
       " 'P79_000075',\n",
       " 'P182_000008',\n",
       " 'P42_000085',\n",
       " 'P48_000132',\n",
       " 'P177_000026',\n",
       " 'P91_000019',\n",
       " 'P77_000000',\n",
       " 'P2_000102',\n",
       " 'P153_000058',\n",
       " 'P191_000016',\n",
       " 'P192_000032',\n",
       " 'P167_000000',\n",
       " 'P184_000101',\n",
       " 'P0_000058',\n",
       " 'P163_000068',\n",
       " 'P27_000126',\n",
       " 'P124_000023',\n",
       " 'P13_000061',\n",
       " 'P26_000069',\n",
       " 'P35_000046',\n",
       " 'P158_000097',\n",
       " 'P42_000000',\n",
       " 'P79_000076',\n",
       " 'P155_000046',\n",
       " 'P34_000049',\n",
       " 'P21_000036',\n",
       " 'P185_000007',\n",
       " 'P124_000106',\n",
       " 'P111_000109',\n",
       " 'P147_000001',\n",
       " 'P154_000048',\n",
       " 'P87_000053',\n",
       " 'P137_000031',\n",
       " 'P131_000011',\n",
       " 'P42_000108',\n",
       " 'P186_000013',\n",
       " 'P184_000177',\n",
       " 'P59_000097',\n",
       " 'P103_000023',\n",
       " 'P107_000047',\n",
       " 'P48_000044',\n",
       " 'P127_000005',\n",
       " 'P192_000006',\n",
       " 'P11_000130',\n",
       " 'P160_000027',\n",
       " 'P126_000120',\n",
       " 'P137_000030',\n",
       " 'P0_000063',\n",
       " 'P194_000071',\n",
       " 'P160_000020',\n",
       " 'P5_000098',\n",
       " 'P42_000024',\n",
       " 'P135_000023',\n",
       " 'P185_000051',\n",
       " 'P96_000027',\n",
       " 'P19_000035',\n",
       " 'P123_000019',\n",
       " 'P64_000008',\n",
       " 'P126_000150',\n",
       " 'P55_000000',\n",
       " 'P0_000068',\n",
       " 'P19_000043',\n",
       " 'P13_000036',\n",
       " 'P8_000017',\n",
       " 'P89_000028',\n",
       " 'P199_000007',\n",
       " 'P128_000037',\n",
       " 'P147_000099',\n",
       " 'P105_000009',\n",
       " 'P126_000094',\n",
       " 'P101_000007',\n",
       " 'P123_000006',\n",
       " 'P13_000030',\n",
       " 'P154_000000',\n",
       " 'P43_000039',\n",
       " 'P177_000030',\n",
       " 'P128_000000',\n",
       " 'P12_000030',\n",
       " 'P135_000068',\n",
       " 'P91_000008',\n",
       " 'P44_000010',\n",
       " 'P109_000064',\n",
       " 'P99_000050',\n",
       " 'P111_000051',\n",
       " 'P88_000035',\n",
       " 'P182_000000',\n",
       " 'P65_000066',\n",
       " 'P193_000002',\n",
       " 'P129_000043',\n",
       " 'P10_000003',\n",
       " 'P128_000044',\n",
       " 'P111_000073',\n",
       " 'P22_000069',\n",
       " 'P200_000005',\n",
       " 'P10_000006',\n",
       " 'P59_000076',\n",
       " 'P6_000030',\n",
       " 'P160_000006',\n",
       " 'P5_000033',\n",
       " 'P65_000083',\n",
       " 'P168_000103',\n",
       " 'P33_000076',\n",
       " 'P90_000109',\n",
       " 'P60_000033',\n",
       " 'P171_000032',\n",
       " 'P33_000100',\n",
       " 'P15_000035',\n",
       " 'P110_000008',\n",
       " 'P181_000078',\n",
       " 'P176_000002',\n",
       " 'P113_000037',\n",
       " 'P145_000072',\n",
       " 'P190_000094',\n",
       " 'P79_000051',\n",
       " 'P183_000054',\n",
       " 'P91_000040',\n",
       " 'P173_000024',\n",
       " 'P107_000043',\n",
       " 'P142_000010',\n",
       " 'P48_000025',\n",
       " 'P6_000033',\n",
       " 'P147_000111',\n",
       " 'P184_000176',\n",
       " 'P155_000003',\n",
       " 'P83_000096',\n",
       " 'P74_000059',\n",
       " 'P201_000065',\n",
       " 'P27_000038',\n",
       " 'P176_000094',\n",
       " 'P42_000154',\n",
       " 'P200_000004',\n",
       " 'P198_000036',\n",
       " 'P134_000036',\n",
       " 'P2_000047',\n",
       " 'P117_000034',\n",
       " 'P21_000042',\n",
       " 'P89_000023',\n",
       " 'P142_000000',\n",
       " 'P191_000018',\n",
       " 'P6_000050',\n",
       " 'P157_000031',\n",
       " 'P79_000024',\n",
       " 'P3_000025',\n",
       " 'P1_000018',\n",
       " 'P202_000005',\n",
       " 'P83_000051',\n",
       " 'P147_000000',\n",
       " 'P85_000016',\n",
       " 'P124_000045',\n",
       " 'P84_000048',\n",
       " 'P33_000117',\n",
       " 'P165_000013',\n",
       " 'P91_000037',\n",
       " 'P134_000059',\n",
       " 'P15_000008',\n",
       " 'P144_000063',\n",
       " 'P107_000079',\n",
       " 'P124_000009',\n",
       " 'P17_000034',\n",
       " 'P30_000050',\n",
       " 'P60_000062',\n",
       " 'P70_000021',\n",
       " 'P96_000009',\n",
       " 'P36_000006',\n",
       " 'P65_000090',\n",
       " 'P19_000087',\n",
       " 'P106_000024',\n",
       " 'P174_000031',\n",
       " 'P39_000001',\n",
       " 'P77_000048',\n",
       " 'P158_000092',\n",
       " 'P19_000029',\n",
       " 'P123_000136',\n",
       " 'P177_000042',\n",
       " 'P21_000095',\n",
       " 'P1_000047',\n",
       " 'P48_000010',\n",
       " 'P166_000043',\n",
       " 'P199_000046',\n",
       " 'P112_000164',\n",
       " 'P158_000118',\n",
       " 'P79_000122',\n",
       " 'P37_000004',\n",
       " 'P15_000076',\n",
       " 'P104_000008',\n",
       " 'P61_000028',\n",
       " 'P180_000110',\n",
       " 'P95_000031',\n",
       " 'P135_000018',\n",
       " 'P193_000001',\n",
       " 'P85_000173',\n",
       " 'P11_000033',\n",
       " 'P201_000023',\n",
       " 'P171_000008',\n",
       " 'P11_000081',\n",
       " 'P105_000012',\n",
       " 'P80_000026',\n",
       " 'P27_000042',\n",
       " 'P27_000032',\n",
       " 'P156_000048',\n",
       " 'P19_000042',\n",
       " 'P0_000043',\n",
       " 'P182_000032',\n",
       " 'P60_000037',\n",
       " 'P44_000030',\n",
       " 'P184_000170',\n",
       " 'P134_000135',\n",
       " 'P150_000079',\n",
       " 'P77_000039',\n",
       " 'P8_000050',\n",
       " 'P147_000130',\n",
       " 'P163_000031',\n",
       " 'P163_000074',\n",
       " 'P168_000018',\n",
       " 'P169_000011',\n",
       " 'P180_000165',\n",
       " 'P160_000052',\n",
       " 'P168_000068',\n",
       " 'P79_000154',\n",
       " 'P106_000039',\n",
       " 'P111_000012',\n",
       " 'P169_000025',\n",
       " 'P6_000064',\n",
       " 'P103_000019',\n",
       " 'P0_000012',\n",
       " 'P165_000068',\n",
       " 'P174_000003',\n",
       " 'P123_000050',\n",
       " 'P34_000018',\n",
       " 'P112_000054',\n",
       " 'P46_000185',\n",
       " 'P111_000176',\n",
       " 'P96_000062',\n",
       " 'P45_000001',\n",
       " 'P39_000003',\n",
       " 'P79_000144',\n",
       " 'P201_000047',\n",
       " 'P97_000033',\n",
       " 'P105_000020',\n",
       " 'P67_000011',\n",
       " 'P181_000050',\n",
       " 'P117_000090',\n",
       " 'P8_000046',\n",
       " 'P104_000038',\n",
       " 'P30_000051',\n",
       " 'P134_000097',\n",
       " 'P124_000018',\n",
       " 'P198_000039',\n",
       " 'P48_000077',\n",
       " 'P35_000179',\n",
       " 'P168_000087',\n",
       " 'P156_000058',\n",
       " 'P85_000017',\n",
       " 'P28_000014',\n",
       " 'P181_000102',\n",
       " 'P25_000035',\n",
       " 'P111_000032',\n",
       " 'P77_000031',\n",
       " 'P88_000058',\n",
       " 'P21_000055',\n",
       " 'P149_000056',\n",
       " 'P136_000014',\n",
       " 'P193_000021',\n",
       " 'P128_000085',\n",
       " 'P97_000044',\n",
       " 'P45_000032',\n",
       " 'P25_000029',\n",
       " 'P163_000048',\n",
       " 'P107_000065',\n",
       " 'P21_000030',\n",
       " 'P31_000018',\n",
       " 'P94_000028',\n",
       " 'P122_000034',\n",
       " 'P198_000053',\n",
       " 'P96_000123',\n",
       " 'P29_000021',\n",
       " 'P171_000015',\n",
       " 'P50_000010',\n",
       " 'P11_000068',\n",
       " 'P59_000066',\n",
       " 'P176_000007',\n",
       " 'P115_000023',\n",
       " 'P0_000044',\n",
       " 'P176_000035',\n",
       " 'P130_000006',\n",
       " 'P44_000005',\n",
       " 'P180_000151',\n",
       " 'P38_000034',\n",
       " 'P198_000026',\n",
       " 'P21_000062',\n",
       " 'P94_000083',\n",
       " 'P106_000010',\n",
       " 'P36_000000',\n",
       " 'P61_000005',\n",
       " 'P186_000069',\n",
       " 'P191_000022',\n",
       " 'P11_000047',\n",
       " 'P147_000078',\n",
       " 'P126_000022',\n",
       " 'P2_000126',\n",
       " 'P80_000020',\n",
       " 'P65_000087',\n",
       " 'P153_000054',\n",
       " 'P165_000035',\n",
       " 'P112_000141',\n",
       " 'P50_000034',\n",
       " 'P111_000095',\n",
       " 'P90_000162',\n",
       " 'P111_000016',\n",
       " 'P2_000035',\n",
       " 'P186_000028',\n",
       " 'P156_000051',\n",
       " 'P21_000059',\n",
       " 'P160_000014',\n",
       " 'P112_000115',\n",
       " 'P176_000047',\n",
       " 'P134_000157',\n",
       " 'P85_000168',\n",
       " 'P182_000035',\n",
       " 'P115_000058',\n",
       " 'P121_000060',\n",
       " 'P184_000066',\n",
       " 'P72_000017',\n",
       " 'P27_000051',\n",
       " 'P162_000023',\n",
       " 'P122_000022',\n",
       " 'P99_000031',\n",
       " 'P184_000084',\n",
       " 'P134_000120',\n",
       " 'P157_000052',\n",
       " 'P21_000092',\n",
       " 'P88_000063',\n",
       " 'P59_000152',\n",
       " 'P154_000043',\n",
       " 'P42_000158',\n",
       " 'P158_000125',\n",
       " ...]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.DLMICustomDataset at 0x7f7853364220>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretraining the resnet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import YourDataset  # 自定义数据集类，需根据实际情况修改\n",
    "\n",
    "# 定义预处理转换\n",
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 加载数据集\n",
    "dataset = YourDataset(\n",
    "    root=\"path/to/your/dataset\", transform=preprocess\n",
    ")  # 修改为你的数据集路径\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# 加载预训练的ResNet模型\n",
    "resnet = models.resnet18(pretrained=True)\n",
    "\n",
    "# 冻结参数，只更新最后一层\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 替换最后一层（全连接层）为自定义的嵌入层\n",
    "num_features = resnet.fc.in_features\n",
    "embedding_size = 128  # 嵌入向量的维度\n",
    "resnet.fc = nn.Linear(num_features, embedding_size)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.TripletMarginLoss()  # 使用三元组损失\n",
    "optimizer = torch.optim.Adam(resnet.parameters(), lr=0.001)\n",
    "\n",
    "# 训练模型\n",
    "num_epochs = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "resnet.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for images, labels in dataloader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # 正向传播\n",
    "        embeddings = resnet(images)\n",
    "\n",
    "        # 计算损失\n",
    "        loss = criterion(embeddings, labels)\n",
    "\n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(dataset)\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# 保存模型\n",
    "torch.save(resnet.state_dict(), \"resnet18_embedding_model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meeg-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
